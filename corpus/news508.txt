谷歌新博客：探索大规模多语言，大规模神经机器翻译

来自谷歌的研究团队近日在名为「大规模的多语言神经机器翻译：发现与挑战」及其后续论文中，通过包括英语以内的 100 多种语言以及 50 亿以上的参数训练超过二十五亿个句子的单一神经机器翻译（NMT）模型，从而突破了多语言 NMT 的研究极限。研究人员进而得出了一种用于大规模多语言，大规模神经机器翻译（M4）的方法，该方法在低资源语言和高资源语言上都显示出质量上的改进，并且可以轻松地适应单个域/语言，同时在跨语言的下游传输任务上显示出巨大的功效。

