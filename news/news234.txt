DeepMind 新研究论文，「为强化学习稳定 Transformer」

近日，DeepMind一篇名为“为强化学习稳定Transformer”在社交媒体上引发广泛关注。该研究论文概述道：在这项工作中，我们证明了标准的 Transformer 架构难以优化，这是以前在监督式学习环境中观察所得，但是对于强化学习目标尤其明显。我们建议对体系结构进行修改，以大幅提高原始 Transformer 的稳定性和学习速度。

