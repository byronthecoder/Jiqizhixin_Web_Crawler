谷歌新论文：使用统一的文本到文本转换器探索迁移学习的局限性

近日，来自谷歌的研究人员发布了一篇名为「使用统一的文本到文本转换器探索迁移学习的局限性」的新论文。以下是该研究的完整摘要分享：转移学习是一种在自然语言处理（NLP）中强大的技术，在这种模型中，模型首先要针对数据丰富的任务进行预训练，然后再针对下游任务进行微调。转移学习的有效性引起了方法，方法和实践的多样性。在本研究中，研究人员通过引入一个统一的框架来探索 NLP 的转移学习技术的前景，该框架将每种语言问题都转换为文本到文本格式。研究团队系统研究比较了数十种语言理解任务中的预训练目标，体系结构，未标记的数据集，传输方法和其他因素。通过将他们从规模探索中得到的洞察与规模以及新的「巨大的干净爬行的语料库」相结合，研究团队对在许多涵盖了摘要，问题回答，文本分类等内容的基准上获得了最先进的结果。为了促进 NLP 迁移学习的未来工作，研究团队还发布了数据集，预训练的模型和代码。GitHub 传送门：bit.ly/31OSHiN

